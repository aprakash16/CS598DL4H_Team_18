{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlv6knX04FiY"
      },
      "source": [
        "# Clone GitHub repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sfk8Zrul_E8V",
        "outputId": "0e45eb25-4940-4162-abec-3816639bd8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CS598DL4H_Team_18\n",
            "From https://github.com/aprakash16/CS598DL4H_Team_18\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "repo_dir = '/content/CS598DL4H_Team_18'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(repo_dir):\n",
        "    # If it exists, navigate to the repository directory and pull changes\n",
        "    %cd $repo_dir\n",
        "    !git pull origin main\n",
        "else:\n",
        "    # If it doesn't exist, create it and clone the repository\n",
        "    os.makedirs(repo_dir)\n",
        "    %cd $repo_dir\n",
        "    !git clone https://github.com/aprakash16/CS598DL4H_Team_18.git .\n",
        "    !mkdir data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "* <strong>Background</strong> <br>\n",
        "The study is based on the paper Barbieri, S., Kemp, J., Perez-Concha, O. et al. “Benchmarking Deep Learning Architectures for Predicting Readmission to the ICU and Describing Patients-at-Risk”, Sci Rep 10, 1111 (2020) https://doi.org/10.1038/s41598-020-58053-z. The original authors have provided the code <a href=\"https://github.com/sebbarb/time_aware_attention/tree/master\">here</a>. We will use the existing code of the original authors so that, there are minimum side effects. Our plan includes to add two ablations mentioned in *Scope of Reproducibility* section.\n",
        "<br>In the realm of healthcare, predicting readmission to the Intensive Care Unit (ICU) plays a crucial role in improving patient outcomes and optimizing resource allocation. This problem falls under the domain of disease readmission prediction, a critical area in healthcare analytics. The ability to accurately forecast which patients are at risk of being readmitted to the ICU within a certain timeframe is essential for healthcare providers to intervene early and provide appropriate care, ultimately reducing healthcare costs and improving patient care quality.\n",
        "\n",
        "* <strong>Importance:</strong> <br>\n",
        "Solving the challenge of predicting ICU readmissions holds significant importance in healthcare systems worldwide. ICU readmissions not only impact patient health but also contribute to avoidable costs within the healthcare system. By accurately identifying patients at risk of readmission, healthcare providers can implement preventive measures, allocate resources efficiently, and enhance patient care strategies. However, the complexity of the problem lies in the dynamic nature of patient health data, the need for feature engineering from diverse sources, and the requirement for interpretable models to guide clinical decision-making.\n",
        "\n",
        "* <strong>State of the Art </strong><br>\n",
        "Current state-of-the-art methods for predicting ICU readmissions often rely on traditional scoring systems like APACHE, SAPS, and OASIS, which have shown moderate predictive power. However, the application of novel machine learning algorithms to electronic medical record (EMR) data has the potential to enhance prediction accuracy and provide more personalized insights into patient outcomes. Deep learning architectures, attention mechanisms, recurrent neural networks, and Bayesian inference have emerged as promising tools for tackling the challenges of ICU readmission prediction.\n",
        "\n",
        "* <strong>Paper Explanation</strong><br>\n",
        "The paper proposes a comprehensive comparison of deep learning architectures for predicting ICU readmissions within 30 days of discharge, leveraging attention-based models for interpretability. The innovation lies in the application of neural ordinary differential equations (ODEs) to model the evolving relevance of medical codes over time, providing a dynamic perspective on patient health trajectories. The proposed method demonstrates competitive predictive accuracy across various neural network architectures, with attention-based models offering interpretability at a marginal cost in accuracy.\n",
        "\n",
        "* <strong>Contribution</strong><br>\n",
        "This paper's contribution to the research regime is significant as it not only benchmarks deep learning models for ICU readmission prediction but also emphasizes the importance of interpretable models in healthcare analytics. By showcasing the effectiveness of attention-based networks and Bayesian inference in predicting ICU readmissions, the study paves the way for more accurate and transparent predictive models in clinical settings, thereby enhancing patient care and healthcare system efficiency.\n",
        "\n",
        "* <strong>Public repository link (Github)</strong> <br>\n",
        "The .ipynb file along with the supplementary code can be found in <a href = \"https://github.com/aprakash16/CS598DL4H_Team_18\">Github repository</a>. This repository will not contain the original MIMIC-III datasets (raw data) due to license and privacy restrictions. However, you can download and place the data when you clone the repository based on the steps given Methodology - Data section. <br>\n",
        "If you wish to reproduce this in the local isolated environment u=you can you this <a href=\"https://github.com/aprakash16/CS598DL4H_Team_18/blob/main/Dockerfile\">Dockerfile</a>.\n",
        "* **Please note:** The same supplementary code which is uploaded in Github has also been placed in google drive for demonstration purpose only. Hence, a subset of the data has been placed in Google drive. The reason for using Google drive is some files (eg. data_arrays.npz - 426MB) are very large and hence cannot be uploaded on Github (max file size limit is 25 MB). PhysioNet has provided public access to MIMIC-III demo data (https://physionet.org/content/mimiciii-demo/1.4/) which has open-access and we will be using for demonstration here. However, the original work has been done on the local setup with the full version of the MIMIC-III dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility\n",
        "\n",
        "The following hypotheses will be tested:\n",
        "\n",
        "1. Hypothesis 1: Recurrent models outperform models that only use attention layers by a small margin.\n",
        "2. Hypothesis 2: Adding attention layers to deep neural networks may improve the model's interpretability but does not degrade average precision.\n",
        "\n",
        "\n",
        "Following ablations have been planned as well in order to observe the impact of eliminating timestamped code from model's embeddings:\n",
        "\n",
        "1. RNN: Embeddings are passed into RNN layers directly.\n",
        "2. RNN + Attention: Embeddings are provided to RNN layers, followed by the application of attention to the RNN outputs\n",
        "\n",
        "**Please note:** This paper compared the performance of 14 models. These are computationally intensive and require significant resources for training and inference. Two ablations will be added as well. This is very time consuming. Also, due to resource limitation, it is be extremely difficult to run all the models. Hence, one model will be chosen to reproduce:\n",
        "1. ODE + RNN + Attention: As per the paper, this model had the highest performance in terms of accuracy as compared to the other deep learning architectures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "outputs": [],
      "source": [
        "# import  packages\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torchdiffeq\n",
        "except ImportError:\n",
        "    print(\"torchdiffeq is not installed. Installing...\")\n",
        "    !pip install torchdiffeq\n",
        "    import torchdiffeq"
      ],
      "metadata": {
        "id": "30b9oSW9LIXJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "\n",
        "  * <strong>Source of the data:</strong> <a href=\"https://physionet.org/content/mimiciii/1.4/\">MIMIC-III Clinical Database 1.4</a> data will be used. It is available on <a href=\"https://physionet.org/\">PhysioNet</a>. For accessing the data, a request needs to be raised. Once the data is available, place the datasets in \"mimic-iii-clinical-database-1.4\" folder at the root of the repository. Please note, you will need to unzip those files as most of them would be downloaded in .gz file format. Due to health data license and privacy concern, this dataset has not been directly placed in the public repository.\n",
        "\n",
        "  * <strong>Dataset details:</strong> The dataset includes patient health data with 61,532 ICU stays as well as 46,476 critical care stays, between year 2001 and 2012. The database contains 26 tables. They are linked by identifiers with suffix'_ID'.\n",
        "  * <strong>Data process:</strong>\n",
        "    - Patients who died during the ICU stay, were not adults (18 years or older) at the time of discharge, or died within 30 days from discharge without being readmitted to the ICU were excluded from the analysis. This step involved filtering out these cases from the dataset to focus on the target population.\n",
        "    - The final dataset comprised ICU stays labeled as either positive (indicating readmission within 30 days) or negative (indicating no readmission within 30 days). This class labeling allowed for the classification task of predicting ICU readmissions.\n",
        "    - The patients were randomly subdivided into training and validation sets (90%) and a separate test set (10%). This splitting was based on patient identifiers to prevent information leaks between the datasets and ensure the models were evaluated on unseen data.\n",
        "    - The dataset underwent further refinement steps, such as handling missing values, encoding categorical variables, normalizing numerical features, and balancing the class distribution if necessary. These steps are common in data preprocessing to ensure the data is suitable for training machine learning models.\n",
        "    - Feature engineering was performed to extract relevant information from the EMR data, and transform existing features to improve the predictive performance of the models. This step involved deriving additional variables and aggregating information from different sources within the EMR data.\n",
        "  * <strong>Step for pre-processing data:</strong> We will refer to the code provided by the authors. Please refer to https://github.com/aprakash16/CS598DL4H_Team_18/tree/main/related_code. Each data pre-processing script took approximately 7 minutes 26 seconds to run (total time = ). Hence, due to time-constraint, the pre-processing scripts have been executed offline. The pre-processed data has not been provided in the repository as it will violate the data privacy policies and HIPAA regulations.\n",
        "The following data pre-processing scripts can be found in 'related_code' folder in Github repository:\n",
        "1. 1_preprocessing_ICU_PAT_ADMIT.py\n",
        "2. 2_preprocessing_reduce_charts.py\n",
        "3. 3_preprocessing_reduce_outputs.py\n",
        "4. 4_preprocessing_merge_charts_outputs.py\n",
        "5. 5_preprocessing_CHARTS_PRESCRIPTIONS.py\n",
        "6. 6_preprocessing_DIAGNOSES_PROCEDURES.py\n",
        "7. 7_preprocessing_create_arrays.py\n",
        "\n",
        "Once, you have placed the unzipped the MIMIC-III datasets and placed them in \"mimic-iii-clinical-database-1.4\", open shell in the root folder and execute the following commands for pre-processing the data.\n",
        "\n",
        "**Please note:** The following snippet is only for the demonstration purpose and the subset of the MIMIC-III dataset (open-access provided by PhysioNet for demo purposes) has been used. The training has been done originally on the full version of the MIMIC-III dataset.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7tNNH96lkMrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "99340d23-b043-493b-ce5f-fb83f91ae8d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas version 1.3.3 is already installed.\n"
          ]
        }
      ],
      "source": [
        "#The original author's code is compatible with pandas 1.3.3 version. It does not work with higher versions\n",
        "import pandas as pd\n",
        "\n",
        "desired_version = '1.3.3'\n",
        "\n",
        "if pd.__version__ != desired_version:\n",
        "    print(f\"Detected Pandas version: {pd.__version__}. Uninstalling...\")\n",
        "    !pip uninstall pandas -y\n",
        "    print(f\"Installing Pandas version: {desired_version}...\")\n",
        "    !pip install pandas=={desired_version}\n",
        "else:\n",
        "    print(f\"Pandas version {desired_version} is already installed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The following code has been commented as it is not required to be executed in this notebook. Please uncomment before executing the commands.\n",
        "#The pre-processed data has been trained offline and the trained model checkpoints are available to load and evaluate in the github repository.\n",
        "\n",
        "!python3 related_code/1_preprocessing_ICU_PAT_ADMIT.py\n",
        "!python3 related_code/2_preprocessing_reduce_charts.py\n",
        "!python3 related_code/3_preprocessing_reduce_outputs.py\n",
        "!python3 related_code/4_preprocessing_merge_charts_outputs.py\n",
        "!python3 related_code/5_preprocessing_CHARTS_PRESCRIPTIONS.py\n",
        "!python3 related_code/6_preprocessing_DIAGNOSES_PROCEDURES.py\n",
        "!python3 related_code/7_preprocessing_create_arrays.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UOKyf_WB--rj",
        "outputId": "be699380-be3b-4dba-c868-871579aafe01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load ICU stays...\n",
            "-----------------------------------------\n",
            "Load patients...\n",
            "-----------------------------------------\n",
            "Load admissions...\n",
            "-----------------------------------------\n",
            "Load services...\n",
            "-----------------------------------------\n",
            "Link icustays and patients tables...\n",
            "Compute number of recent admissions...\n",
            "100% 77/77 [00:00<00:00, 481.11it/s]\n",
            "-----------------------------------------\n",
            "Link icu_pat and admissions tables...\n",
            "SUBJECT_ID               0\n",
            "HADM_ID                  0\n",
            "ICUSTAY_ID               0\n",
            "INTIME                   0\n",
            "OUTTIME                  0\n",
            "LOS                      0\n",
            "GENDER_M                 0\n",
            "NUM_RECENT_ADMISSIONS    0\n",
            "AGE                      0\n",
            "POSITIVE                 0\n",
            "ADMITTIME                0\n",
            "ADMISSION_TYPE           0\n",
            "ADMISSION_LOCATION       0\n",
            "INSURANCE                0\n",
            "MARITAL_STATUS           5\n",
            "ETHNICITY                0\n",
            "dtype: int64\n",
            "Some data cleaning on admissions...\n",
            "-----------------------------------------\n",
            "Link services table...\n",
            "-----------------------------------------\n",
            "Total pos 11\n",
            "Total neg 70\n",
            "count    11.000000\n",
            "mean      3.091173\n",
            "std       1.831840\n",
            "min       1.147400\n",
            "25%       1.937150\n",
            "50%       2.825800\n",
            "75%       3.591800\n",
            "max       7.902000\n",
            "Name: LOS, dtype: float64\n",
            "count    70.000000\n",
            "mean      3.683767\n",
            "std       5.383570\n",
            "min       0.654900\n",
            "25%       1.179175\n",
            "50%       1.902050\n",
            "75%       3.282575\n",
            "max      31.123501\n",
            "Name: LOS, dtype: float64\n",
            "count    11.000000\n",
            "mean      0.431755\n",
            "std       0.839130\n",
            "min       0.000000\n",
            "25%       0.000347\n",
            "50%       0.000694\n",
            "75%       0.481597\n",
            "max       2.802778\n",
            "Name: PRE_ICU_LOS, dtype: float64\n",
            "count    70.000000\n",
            "mean      0.476002\n",
            "std       1.223940\n",
            "min       0.000000\n",
            "25%       0.000694\n",
            "50%       0.001042\n",
            "75%       0.138021\n",
            "max       5.690278\n",
            "Name: PRE_ICU_LOS, dtype: float64\n",
            "count    11.0\n",
            "mean     71.4\n",
            "std      13.6\n",
            "min      53.5\n",
            "25%      58.5\n",
            "50%      77.0\n",
            "75%      82.4\n",
            "max      91.5\n",
            "Name: AGE, dtype: float64\n",
            "count    70.0\n",
            "mean     70.3\n",
            "std      13.6\n",
            "min      27.9\n",
            "25%      63.8\n",
            "50%      70.8\n",
            "75%      80.8\n",
            "max      91.5\n",
            "Name: AGE, dtype: float64\n",
            "count    11.0\n",
            "mean      0.7\n",
            "std       1.5\n",
            "min       0.0\n",
            "25%       0.0\n",
            "50%       0.0\n",
            "75%       1.0\n",
            "max       5.0\n",
            "Name: NUM_RECENT_ADMISSIONS, dtype: float64\n",
            "count    70.0\n",
            "mean      0.6\n",
            "std       1.3\n",
            "min       0.0\n",
            "25%       0.0\n",
            "50%       0.0\n",
            "75%       1.0\n",
            "max       6.0\n",
            "Name: NUM_RECENT_ADMISSIONS, dtype: float64\n",
            "   COUNTS  PERC\n",
            "1       9  81.8\n",
            "0       2  18.2\n",
            "   COUNTS  PERC\n",
            "1      41  58.6\n",
            "0      29  41.4\n",
            "               COUNTS   PERC\n",
            "Other/Unknown      11  100.0\n",
            "               COUNTS   PERC\n",
            "Other/Unknown      70  100.0\n",
            "               COUNTS   PERC\n",
            "Other/Unknown      11  100.0\n",
            "               COUNTS   PERC\n",
            "Other/Unknown      70  100.0\n",
            "               COUNTS   PERC\n",
            "Other/Unknown      11  100.0\n",
            "               COUNTS   PERC\n",
            "Other/Unknown      70  100.0\n",
            "               COUNTS   PERC\n",
            "Other/Unknown      11  100.0\n",
            "               COUNTS   PERC\n",
            "Other/Unknown      70  100.0\n",
            "   COUNTS  PERC\n",
            "0      10  90.9\n",
            "1       1   9.1\n",
            "   COUNTS  PERC\n",
            "0      64  91.4\n",
            "1       6   8.6\n",
            "-----------------------------------------\n",
            "Save...\n",
            "-----------------------------------------\n",
            "Load item definitions\n",
            "GCS_EYE_OPENING\n",
            "       ITEMID                LABEL UNITNAME\n",
            "184       184          Eye Opening      NaN\n",
            "9520   220739    GCS - Eye Opening      NaN\n",
            "11329  226756  GCSEyeApacheIIValue      NaN\n",
            "11500  227011      GCSEye_ApacheIV      NaN\n",
            "GCS_VERBAL_RESPONSE\n",
            "       ITEMID                   LABEL UNITNAME\n",
            "671       723         Verbal Response      NaN\n",
            "9639   223900   GCS - Verbal Response      NaN\n",
            "11331  226758  GCSVerbalApacheIIValue      NaN\n",
            "11503  227014      GCSVerbal_ApacheIV      NaN\n",
            "GCS_MOTOR_RESPONSE\n",
            "       ITEMID                  LABEL UNITNAME\n",
            "417       454         Motor Response      NaN\n",
            "9640   223901   GCS - Motor Response      NaN\n",
            "11330  226757  GCSMotorApacheIIValue      NaN\n",
            "11501  227012      GCSMotor_ApacheIV      NaN\n",
            "GCS_TOTAL\n",
            "       ITEMID             LABEL UNITNAME\n",
            "198       198         GCS Total      NaN\n",
            "11328  226755  GcsApacheIIScore     None\n",
            "DIASTOLIC_BP\n",
            "       ITEMID                                  LABEL UNITNAME\n",
            "4826     8364                        ABP [Diastolic]      NaN\n",
            "4830     8368                Arterial BP [Diastolic]      NaN\n",
            "4902     8440                  Manual BP [Diastolic]      NaN\n",
            "4903     8441                        NBP [Diastolic]      NaN\n",
            "4952     8502                    BP Cuff [Diastolic]      NaN\n",
            "4953     8503                BP Left Arm [Diastolic]      NaN\n",
            "4956     8506               BP Right Arm [Diastolic]      NaN\n",
            "5016     8555             Arterial BP #2 [Diastolic]      NaN\n",
            "9529   220051      Arterial Blood Pressure diastolic     mmHg\n",
            "9547   220180  Non Invasive Blood Pressure diastolic     mmHg\n",
            "10236  224643   Manual Blood Pressure Diastolic Left     mmHg\n",
            "10500  225310                       ART BP Diastolic     mmHg\n",
            "11430  227242  Manual Blood Pressure Diastolic Right     mmHg\n",
            "SYSTOLIC_BP\n",
            "       ITEMID                                 LABEL UNITNAME\n",
            "31          6                        ABP [Systolic]      NaN\n",
            "56         51                Arterial BP [Systolic]      NaN\n",
            "407       442                  Manual BP [Systolic]      NaN\n",
            "418       455                        NBP [Systolic]      NaN\n",
            "928      3313                    BP Cuff [Systolic]      NaN\n",
            "930      3315                BP Left Arm [Systolic]      NaN\n",
            "936      3321               BP Right Arm [Systolic]      NaN\n",
            "4324     6701             Arterial BP #2 [Systolic]      NaN\n",
            "9528   220050      Arterial Blood Pressure systolic     mmHg\n",
            "9546   220179  Non Invasive Blood Pressure systolic     mmHg\n",
            "9862   224167   Manual Blood Pressure Systolic Left     mmHg\n",
            "10499  225309                       ART BP Systolic     mmHg\n",
            "11431  227243  Manual Blood Pressure Systolic Right     mmHg\n",
            "MEAN_BP\n",
            "       ITEMID                             LABEL UNITNAME\n",
            "57         52                  Arterial BP Mean      NaN\n",
            "408       443              Manual BP Mean(calc)      NaN\n",
            "419       456                          NBP Mean      NaN\n",
            "927      3312                    BP Cuff [Mean]      NaN\n",
            "929      3314                BP Left Arm [Mean]      NaN\n",
            "935      3320               BP Right Arm [Mean]      NaN\n",
            "3463     2293                            ART BP      NaN\n",
            "3464     2294                          ART MEAN      NaN\n",
            "3691     2647                          art mean      NaN\n",
            "4268     6590                  arterial mean #2      NaN\n",
            "4325     6702               Arterial BP Mean #2      NaN\n",
            "4433     6927                  Arterial Mean #3      NaN\n",
            "4726     7620                   BP Rt. Arm Mean      NaN\n",
            "9530   220052      Arterial Blood Pressure mean     mmHg\n",
            "9548   220181  Non Invasive Blood Pressure mean     mmHg\n",
            "10501  225312                       ART BP mean     mmHg\n",
            "HEART_RATE\n",
            "       ITEMID        LABEL UNITNAME\n",
            "211       211   Heart Rate      NaN\n",
            "9524   220045   Heart Rate      bpm\n",
            "11507  227018  HR_ApacheIV      bpm\n",
            "FRACTION_INSPIRED_OXYGEN\n",
            "       ITEMID                 LABEL UNITNAME\n",
            "189       189       FiO2 (Analyzed)      NaN\n",
            "190       190              FiO2 Set      NaN\n",
            "675       727           Vision FiO2      NaN\n",
            "1018     3420                  FIO2      NaN\n",
            "1020     3422           FIO2 [Meas]      NaN\n",
            "1196     1040            BIpap FIO2      NaN\n",
            "1297     1206             HFO FIO2:      NaN\n",
            "3239     1863              HFO-FiO2      NaN\n",
            "3611     2518             HFO- FIO2      NaN\n",
            "3910     2981                  FiO2      NaN\n",
            "4472     7018             ecmo fio2      NaN\n",
            "4486     7041        vapotherm fio2      NaN\n",
            "4699     7570              FIO2 SET      NaN\n",
            "9616   223835  Inspired O2 Fraction     None\n",
            "11327  226754     FiO2ApacheIIValue        %\n",
            "11498  227009     FiO2_ApacheIV_old     None\n",
            "11499  227010         FiO2_ApacheIV        %\n",
            "RESPIRATORY_RATE\n",
            "       ITEMID                           LABEL  UNITNAME\n",
            "570       614               Resp Rate (Spont)       NaN\n",
            "571       615               Resp Rate (Total)       NaN\n",
            "574       618                Respiratory Rate       NaN\n",
            "575       619            Respiratory Rate Set       NaN\n",
            "603       651                 Spon RR (Mech.)       NaN\n",
            "605       653               Spont. Resp. Rate       NaN\n",
            "2299     7884                              RR       NaN\n",
            "2394     8113                      Resp. Rate       NaN\n",
            "2779     3603                       Resp Rate       NaN\n",
            "3252     1884                 Spont Resp rate       NaN\n",
            "4352     6749                        total RR       NaN\n",
            "9550   220210                Respiratory Rate  insp/min\n",
            "10056  224688          Respiratory Rate (Set)  insp/min\n",
            "10057  224689  Respiratory Rate (spontaneous)  insp/min\n",
            "10058  224690        Respiratory Rate (Total)  insp/min\n",
            "10151  224422                        Spont RR       bpm\n",
            "11347  226774                 RRApacheIIValue  insp/min\n",
            "11539  227050                     RR_ApacheIV  insp/min\n",
            "BODY_TEMPERATURE\n",
            "       ITEMID                   LABEL UNITNAME\n",
            "626       676           Temperature C      NaN\n",
            "627       677    Temperature C (calc)      NaN\n",
            "628       678           Temperature F      NaN\n",
            "629       679    Temperature F (calc)      NaN\n",
            "2815     3652       Temp Axillary [F]      NaN\n",
            "2817     3654         Temp Rectal [F]      NaN\n",
            "4297     6643             Temp Rectal      NaN\n",
            "9569   223761  Temperature Fahrenheit       ?F\n",
            "9570   223762     Temperature Celsius       ?C\n",
            "11351  226778       TempApacheIIValue       ?F\n",
            "11543  227054   TemperatureF_ApacheIV       ?F\n",
            "WEIGHT\n",
            "       ITEMID                    LABEL UNITNAME\n",
            "687       763             Daily Weight      NaN\n",
            "2764     3580     Present Weight  (kg)      NaN\n",
            "2765     3581     Present Weight  (lb)      NaN\n",
            "2766     3582     Present Weight  (oz)      NaN\n",
            "2848     3693                Weight Kg      NaN\n",
            "10232  224639             Daily Weight       kg\n",
            "11203  226512    Admission Weight (Kg)       kg\n",
            "11213  226531  Admission Weight (lbs.)     None\n",
            "HEIGHT\n",
            "       ITEMID          LABEL UNITNAME\n",
            "1401     1394  Height Inches      NaN\n",
            "11304  226707         Height     Inch\n",
            "11308  226730    Height (cm)       cm\n",
            "-----------------------------------------\n",
            "Loading Chart Events\n",
            "1it [00:01,  1.34s/it]\n",
            "-----------------------------------------\n",
            "Load item definitions\n",
            "URINE_OUTPUT\n",
            "5358              pacu out pacu urine\n",
            "5371                  urine out other\n",
            "5376          urine out straight cath\n",
            "5390                      urine flush\n",
            "5402                  cath lab output\n",
            "                     ...             \n",
            "11233                   straight cath\n",
            "11297                        or urine\n",
            "11301                      pacu urine\n",
            "11302                        cath lab\n",
            "11655    gu irrigant/urine volume out\n",
            "Name: LABEL, Length: 99, dtype: object\n",
            "-----------------------------------------\n",
            "Loading Output Events\n",
            "Remove admission and discharge days (since data on urine output is incomplete)\n",
            "Load ICU stays...\n",
            "Loading chart events...\n",
            "-----------------------------------------\n",
            "Compute BMI and GCS total...\n",
            "-----------------------------------------\n",
            "Loading output events...\n",
            "-----------------------------------------\n",
            "Create categorical variable...\n",
            "-----------------------------------------\n",
            "Save...\n",
            "-----------------------------------------\n",
            "Save data for logistic regression...\n",
            "Loading icu_pat...\n",
            "-----------------------------------------\n",
            "Load charts and outputs...\n",
            "-----------------------------------------\n",
            "Load prescriptions...\n",
            "-----------------------------------------\n",
            "Link charts/outputs and icu_pat tables...\n",
            "Drop duplicates...\n",
            "Map rare codes to OTHER...\n",
            "-----------------------------------------\n",
            "Save...\n",
            "Loading icu_pat...\n",
            "-----------------------------------------\n",
            "Load admissions...\n",
            "-----------------------------------------\n",
            "Load diagnoses and procedures...\n",
            "-----------------------------------------\n",
            "Link diagnoses/procedures and admissions tables...\n",
            "Link diagnoses/procedures and icu_pat tables...\n",
            "Drop duplicates...\n",
            "Map rare codes to OTHER...\n",
            "-----------------------------------------\n",
            "Save...\n",
            "Loading icu_pat...\n",
            "Loading diagnoses/procedures...\n",
            "Loading charts/prescriptions...\n",
            "-----------------------------------------\n",
            "Create static array...\n",
            "Create label array...\n",
            "Create diagnoses/procedures and charts/prescriptions array...\n",
            "max_count 312\n",
            "Reindex df...\n",
            "done\n",
            "max_count 367\n",
            "Reindex df...\n",
            "done\n",
            "-----------------------------------------\n",
            "Split data into train/validate/test...\n",
            "Get patients corresponding to test ids\n",
            "-----------------------------------------\n",
            "Save...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muyDPFPbozY"
      },
      "source": [
        "##   Model\n",
        "* **ODE+RNN+Attention:** Use neural ODEs to model dynamics in time of embeddings, embeddings are then passed to RNN layers, with attention applied to the RNN outputs.<br>\n",
        "  - ODE Layer: The ODE layer captures the temporal dynamics of the system. This could be represented by the differential equations governing the evolution of the system over time. The model uses techniques such as Neural ODEs (Ordinary Differential Equations) to parameterize the dynamics.<br>\n",
        "  - RNN Layer: The RNN layer processes the sequential data and captures temporal dependencies. In the paper, an LSTM (Long Short-Term Memory) network is used, which is a type of recurrent neural network known for its ability to retain information over long sequences.<br>\n",
        "  - Attention Mechanism: The attention mechanism focuses on relevant parts of the input sequence at each time step. It attends to different time points in the input sequence based on their relevance to the prediction at the current time step.<br>\n",
        "\n",
        "  The following code has been provided for demonstration purpose. The code is available https://github.com/aprakash16/CS598DL4H_Team_18/blob/main/related_code/modules.py."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Code snippet of the model\n",
        "# # Attention Only\n",
        "#   class Net(nn.Module):\n",
        "#     def __init__(self, num_static, num_dp_codes, num_cp_codes):\n",
        "#       super(Net, self).__init__()\n",
        "\n",
        "#       # Embedding dimensions\n",
        "#       self.embed_dp_dim = int(np.ceil(num_dp_codes**0.25))+1\n",
        "#       self.embed_cp_dim = int(np.ceil(num_cp_codes**0.25))+1\n",
        "\n",
        "#       # Embedding layers\n",
        "#       self.embed_dp = nn.Embedding(num_embeddings=num_dp_codes, embedding_dim=self.embed_dp_dim, padding_idx=0)\n",
        "#       self.embed_cp = nn.Embedding(num_embeddings=num_cp_codes, embedding_dim=self.embed_cp_dim, padding_idx=0)\n",
        "\n",
        "#       # ODE layers\n",
        "#       self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "#       self.ode_dp = ODENet(self.device, self.embed_dp_dim, self.embed_dp_dim, output_dim=self.embed_dp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "#       self.ode_cp = ODENet(self.device, self.embed_cp_dim, self.embed_cp_dim, output_dim=self.embed_cp_dim, augment_dim=0, time_dependent=False, non_linearity='softplus', tol=1e-3, adjoint=True)\n",
        "\n",
        "#       # GRU layers\n",
        "#       self.gru_dp_fw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
        "#       self.gru_cp_fw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
        "#       self.gru_dp_bw = nn.GRU(input_size=self.embed_dp_dim, hidden_size=self.embed_dp_dim, num_layers=1, batch_first=True)\n",
        "#       self.gru_cp_bw = nn.GRU(input_size=self.embed_cp_dim, hidden_size=self.embed_cp_dim, num_layers=1, batch_first=True)\n",
        "\n",
        "#       # Attention layers\n",
        "#       self.attention_dp = Attention(embedding_dim=2*self.embed_dp_dim)\n",
        "#       self.attention_cp = Attention(embedding_dim=2*self.embed_cp_dim)\n",
        "\n",
        "#       # Fully connected output\n",
        "#       self.fc_dp  = nn.Linear(2*self.embed_dp_dim, 1)\n",
        "#       self.fc_cp  = nn.Linear(2*self.embed_cp_dim, 1)\n",
        "#       self.fc_all = nn.Linear(num_static + 2, 1)\n",
        "\n",
        "#       # Others\n",
        "#       self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "#     def forward(self, stat, dp, cp, dp_t, cp_t):\n",
        "#       # Embedding\n",
        "#       ## output dim: batch_size x seq_len x embedding_dim\n",
        "#       embedded_dp = self.embed_dp(dp)\n",
        "#       embedded_cp = self.embed_cp(cp)\n",
        "\n",
        "#       # ODE\n",
        "#       ## Round times\n",
        "#       dp_t = torch.round(100*dp_t)/100\n",
        "#       cp_t = torch.round(100*cp_t)/100\n",
        "\n",
        "#       embedded_dp_long = embedded_dp.view(-1, self.embed_dp_dim)\n",
        "#       dp_t_long = dp_t.view(-1)\n",
        "#       dp_t_long_unique, inverse_indices = torch.unique(dp_t_long, sorted=True, return_inverse=True)\n",
        "#       ode_dp_long = self.ode_dp(embedded_dp_long, dp_t_long_unique)\n",
        "#       ode_dp_long = ode_dp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "#       ode_dp = ode_dp_long.view(dp.size(0), dp.size(1), self.embed_dp_dim)\n",
        "\n",
        "#       embedded_cp_long = embedded_cp.view(-1, self.embed_cp_dim)\n",
        "#       cp_t_long = cp_t.view(-1)\n",
        "#       cp_t_long_unique, inverse_indices = torch.unique(cp_t_long, sorted=True, return_inverse=True)\n",
        "#       ode_cp_long = self.ode_cp(embedded_cp_long, cp_t_long_unique)\n",
        "#       ode_cp_long = ode_cp_long[inverse_indices, torch.arange(0, inverse_indices.size(0)), :]\n",
        "#       ode_cp = ode_cp_long.view(cp.size(0), cp.size(1), self.embed_cp_dim)\n",
        "\n",
        "#       ## Dropout\n",
        "#       ode_dp = self.dropout(ode_dp)\n",
        "#       ode_cp = self.dropout(ode_cp)\n",
        "\n",
        "#       # Forward and backward sequences\n",
        "#       ## output dim: batch_size x seq_len x embedding_dim\n",
        "#       ode_dp_fw = ode_dp\n",
        "#       ode_cp_fw = ode_cp\n",
        "#       ode_dp_bw = torch.flip(ode_dp_fw, [1])\n",
        "#       ode_cp_bw = torch.flip(ode_cp_fw, [1])\n",
        "\n",
        "#       # GRU\n",
        "#       ## output dim rnn:        batch_size x seq_len x embedding_dim\n",
        "#       ## output dim rnn_hidden: batch_size x 1 x embedding_dim\n",
        "#       rnn_dp_fw, rnn_hidden_dp_fw = self.gru_dp_fw(ode_dp_fw)\n",
        "#       rnn_cp_fw, rnn_hidden_cp_fw = self.gru_cp_fw(ode_cp_fw)\n",
        "#       rnn_dp_bw, rnn_hidden_dp_bw = self.gru_dp_bw(ode_dp_bw)\n",
        "#       rnn_cp_bw, rnn_hidden_cp_bw = self.gru_cp_bw(ode_cp_bw)\n",
        "#       # concatenate forward and backward\n",
        "#       ## output dim: batch_size x seq_len x 2*embedding_dim\n",
        "#       rnn_dp = torch.cat((rnn_dp_fw, torch.flip(rnn_dp_bw, [1])), dim=-1)\n",
        "#       rnn_cp = torch.cat((rnn_cp_fw, torch.flip(rnn_cp_bw, [1])), dim=-1)\n",
        "\n",
        "#       # Attention\n",
        "#       ## output dim: batch_size x 2*embedding_dim\n",
        "#       attended_dp, weights_dp = self.attention_dp(rnn_dp, (dp > 0).float())\n",
        "#       attended_cp, weights_cp = self.attention_cp(rnn_cp, (cp > 0).float())\n",
        "\n",
        "#       # Scores\n",
        "#       score_dp = self.fc_dp(self.dropout(attended_dp))\n",
        "#       score_cp = self.fc_cp(self.dropout(attended_cp))\n",
        "\n",
        "#       # Concatenate to variable collection\n",
        "#       all = torch.cat((stat, score_dp, score_cp), dim=1)\n",
        "\n",
        "#       # Final linear projection\n",
        "#       out = self.fc_all(self.dropout(all)).squeeze()\n",
        "\n",
        "#       return out, []"
      ],
      "metadata": {
        "id": "UYuCzkR5kt7e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Training"
      ],
      "metadata": {
        "id": "McsCMJB_YeDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training model:** Training ODE+RNN+Attention model took 9 hrs 28 minutes on local machine. Hence, due to time constraint, the model was trained in local setup and the trained model checkpoint has been uploaded in https://github.com/aprakash16/CS598DL4H_Team_18/blob/main/logdir-final/ode_birnn_attention. Please refer to 'final_model.pt' file.\n",
        "\n",
        "**Computational requirements:** Our attempt was to work with Google Colab as it offers runtime types like CPU and GPU but GPU resources are limited in free account. Also, the size of dataset was very large. Hence, we used HP Pavilion Intel(R) Core(TM) i7-10870H CPU @ 2.20GHz with 16GB RAM."
      ],
      "metadata": {
        "id": "vW3qs2VZavZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the code provided by the original authors. Please refer to https://github.com/aprakash16/CS598DL4H_Team_18/blob/main/related_code/train.py for the training model code. To train the model, the following code has been used. As it takes approximately 9.5 hrs to train, we have uploaded the trained model checkpoint in the github repository.\n",
        "\n",
        "For training (full version of MIMIC-III dataset) on local setup:\n",
        "* batch size: 128\n",
        "* number of epochs: 80\n",
        "* dropout rate: 0.5\n",
        "* patience (early stopping): 10\n",
        "* runtime for each epoch: 07:01 - 08:05 minutes\n",
        "* total time to train model: 09:28:20 hrs"
      ],
      "metadata": {
        "id": "we1K3jCoy4YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please note:** For the sake of demonstrating runnable training code, we have taken a subset of the MIMIC-III data which is publicly accessible and have run the training script for it.\n",
        "The final trained model that we are using in *Methodology.Evaluation* section is being done on the full version of the dataset."
      ],
      "metadata": {
        "id": "pJfak-mde0EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/CS598DL4H_Team_18/train_model.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "14Ha2a3id8a2",
        "outputId": "96b2fbbb-56e5-4d97-cc7e-99a573abdf4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) ODE + RNN + Attention\n",
            "2) QUIT\n",
            "Which model do you want to train? 1\n",
            "====================================\n",
            "Training ODE + RNN + Attention (ode_birnn_attention) ...\n",
            "====================================\n",
            "Load data...\n",
            "-----------------------------------------\n",
            "Train...\n",
            "  0% 0/80 [00:00<?, ?it/s]\n",
            "0it [00:00, ?it/s]\u001b[A/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "  2% 2/80 [00:00<00:04, 18.22it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "  6% 5/80 [00:00<00:03, 20.63it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 10% 8/80 [00:00<00:03, 20.74it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 14% 11/80 [00:00<00:03, 20.00it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 16% 13/80 [00:00<00:03, 19.30it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 20% 16/80 [00:00<00:03, 19.92it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 22% 18/80 [00:00<00:03, 19.77it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 26% 21/80 [00:01<00:02, 20.57it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 30% 24/80 [00:01<00:02, 20.30it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 34% 27/80 [00:01<00:02, 20.03it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 38% 30/80 [00:01<00:02, 20.25it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 41% 33/80 [00:01<00:02, 19.58it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 45% 36/80 [00:01<00:02, 20.08it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 49% 39/80 [00:01<00:02, 19.21it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 51% 41/80 [00:02<00:02, 19.12it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 54% 43/80 [00:02<00:02, 18.28it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 56% 45/80 [00:02<00:01, 17.76it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 59% 47/80 [00:02<00:01, 16.88it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 61% 49/80 [00:02<00:01, 16.81it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 64% 51/80 [00:02<00:01, 15.84it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 66% 53/80 [00:02<00:01, 15.93it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 69% 55/80 [00:02<00:01, 16.27it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 71% 57/80 [00:03<00:01, 16.00it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 74% 59/80 [00:03<00:01, 16.54it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 78% 62/80 [00:03<00:00, 18.04it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 81% 65/80 [00:03<00:00, 19.56it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 85% 68/80 [00:03<00:00, 19.85it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 88% 70/80 [00:03<00:00, 19.28it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 90% 72/80 [00:03<00:00, 19.30it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 92% 74/80 [00:03<00:00, 19.17it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            " 96% 77/80 [00:04<00:00, 19.54it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "100% 80/80 [00:04<00:00, 18.89it/s]\n",
            "Saving...\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run this to download the final trained model from Google drive.\n",
        "\n",
        "#The same supplementary code which is uploaded in Github has also been placed in google drive for demonstration purpose only.\n",
        "#Hence, a subset of the data has been placed in Google drive. The reason for using Google drive is some files (eg. data_arrays.npz - 426MB)\n",
        "#are very large and hence cannot be uploaded on Github (max file size limit is 25 MB).\n",
        "try:\n",
        "    import gdown\n",
        "except ImportError:\n",
        "    print(\"gdown is not installed. Installing...\")\n",
        "    !pip install gdown\n",
        "    import gdown\n",
        "\n",
        "import shutil\n",
        "# Define the URL of the public folder\n",
        "url = \"https://drive.google.com/drive/folders/1JHb6mHWo97uug4x0E5N04-2KX6erA4dB\"\n",
        "\n",
        "# Define the path where you want to download the folder\n",
        "output_folder = \"/content/CS598-DL4H-Team18-final\"\n",
        "\n",
        "# Remove the existing folder if it exists\n",
        "shutil.rmtree(output_folder, ignore_errors=True)\n",
        "\n",
        "# Download the folder contents\n",
        "gdown.download_folder(url, output=output_folder, quiet=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IbgZVtaqr6MA",
        "outputId": "1285faea-9f5f-4733-9bea-8971f41accb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder 1FVk__ACdubaHz0_gMugOkXqwLXQ-eUYy data\n",
            "Processing file 19fs8nuUI0ZHWoVQRrs3L6h5cBT2SFTOv data_arrays.npz\n",
            "Processing file 1qcrNWb1GdBX8jct_qZaLBIkgVR3MdIz7 test_ids_patients.pkl\n",
            "Processing file 1kIXfybocTDIhxRsXfshISOyVHrUjMmPF Dockerfile\n",
            "Processing file 1ZtodqJxf2eb8aJsENZEwgg2PtPvr23wi eval_model.sh\n",
            "Retrieving folder 15PP0RBNoWXwyl5fkA0GiOLHG4YjMflLN logdir\n",
            "Retrieving folder 1yeQab3xiwUpqy5nBsdkibCW6L9vvF3ta ode_birnn_attention\n",
            "Processing file 1pRvui7NzpRqiJnP2sx2ued0SueM0cS3i epoch_times.npz\n",
            "Processing file 1pruZTvX5h-jvHBJb8jZzEb-HCCXCLYJH final_model.pt\n",
            "Retrieving folder 12-mvgUxT4JlLzoZR_ak9X1ezYnNNTKEh mimic-iii-clinical-database-1.4\n",
            "Processing file 1dqumtKiLMBR2444NuI2wx650HRS3IHON ADMISSIONS.csv\n",
            "Processing file 1sqOu39urse4dqRLJdfg7Nwfh9XQYe3_U CALLOUT.csv\n",
            "Processing file 1dI3tHD_LcfdDi0xxVWzK2koBkMMvM-qP CAREGIVERS.csv\n",
            "Processing file 1wHQAiBfRw4w0hi_8O8mEGis-ApV0o63B CHARTEVENTS.csv\n",
            "Processing file 1eETouFkHzGO3c1cKn3Z_MNUVSVe8fYIz D_ITEMS.csv\n",
            "Processing file 1GkmohzxqOvsMdUuzgsndwc_nkTQhqLMU DIAGNOSES_ICD.csv\n",
            "Processing file 1xUzZVlwCbiT5aRbGhZwIWAIPc7QP9axy ICUSTAYS.csv\n",
            "Processing file 1Nne2jTU4VbryveHfAAi6LZlCCqDzwcuW OUTPUTEVENTS.csv\n",
            "Processing file 1f6iBzMwPcHIxXQoc2j6hGChAqjoW62oq PATIENTS.csv\n",
            "Processing file 134SvY8X-Odf7l13UuDPyxOrBKagyWEPP PRESCRIPTIONS.csv\n",
            "Processing file 1g9V9pZBszszh6lHwyN2_NUujW0BIOxil PROCEDURES_ICD.csv\n",
            "Processing file 1AJb7I1FOWzO_RPY7HFK5J930Qxrfmf4F SERVICES.csv\n",
            "Processing file 1BsKTKVzov79ZXmQf-8JmWQTEIN5zwQEy SHA256SUMS.txt\n",
            "Processing file 1J-g2RCsI-amIj95YEDFOHbSXp-bDBMLF README.md\n",
            "Retrieving folder 1rjDASBbfxw_LD_15DqMMKYD_YuCfHwEy related_code\n",
            "Retrieving folder 1SBtmr7dcblNqaBVbH0NA856dWXA4tE5H __pycache__\n",
            "Processing file 1Y1uXRPxLIuYCGfaQFBWf3J9h68F6eUA_ 1_preprocessing_ICU_PAT_ADMIT.py\n",
            "Processing file 1HaKx7rV4PXb8Y4IqiBZlrbNc5TqTDlWy 2_preprocessing_reduce_charts.py\n",
            "Processing file 1WOqdUwJHZHs-8mmMtD6ozQQ63rc6H0RO 3_preprocessing_reduce_outputs.py\n",
            "Processing file 1tX_-bkIcUY6p_mfZs0KBxYc6t7qDTH6D 4_preprocessing_merge_charts_outputs.py\n",
            "Processing file 1DPDi7dJNxxoxYDpfn-B4EcWRfCRqyvTN 5_preprocessing_CHARTS_PRESCRIPTIONS.py\n",
            "Processing file 1oMrKKWoEUb9WLwNaQ6Ktctl9kDsMxfhN 6_preprocessing_DIAGNOSES_PROCEDURES.py\n",
            "Processing file 1ZjUIW8PDhDSj0I14dgJhuIrIXCKUhRxB 7_preprocessing_create_arrays.py\n",
            "Processing file 1d8rj1Tba70QGozvC9GZB0-hI0qJf1hOB bayesian_interpretation.py\n",
            "Processing file 1hSFETY2OhJgdCXY1mJx1j-B2CEzmxZuR bayesian_save_embeddings.py\n",
            "Processing file 1o3AO9diTyksNfu_Jwelw5x2rncPcDWOU bayesian_train.py\n",
            "Processing file 1Ibk2slcMzdDk9TyRjmJsby-6jAHj9CQq data_load.py\n",
            "Retrieving folder 1EWzW2h8L7lgwi0_Al801KHS5ldAj5lw2 embeddings\n",
            "Processing file 17SY9JyMy4UFEIEUGa5AaRh8-rEBNGuus emb_weight_cp_6.npy\n",
            "Processing file 1a60NvUXpScCuI9joqXUaf2eCTnQjpk-3 emb_weight_cp_11.npy\n",
            "Processing file 1W1L2w_Zbq3Q2zJk1LQmMeWqmrrfvG5Ay emb_weight_dp_7.npy\n",
            "Processing file 1oB3Cd2ZSj4iQ858HXxpUSIGc_sBdnCm- emb_weight_dp_13.npy\n",
            "Processing file 13zm26rDBWhyCYov2mnM4dEBtISY-5HLv helper_eval_model.py\n",
            "Processing file 1_Km0OmLnqbofBnzIXZoc8cSLFPwNMv5L helper_train_model.py\n",
            "Processing file 1EfXKHvamwSV2R4OiTBApkX9xIn9MStCD hyperparameters.py\n",
            "Processing file 1NnVzdM0HyUuEpvJy2XsO2K1blMJbFFa5 modules_ode.py\n",
            "Processing file 1KkEIepYUkfhkPkSgxqGvPdzvhIdolt0A modules.py\n",
            "Processing file 15Z_1uOxkB7hF-zPPkc5EaoWIitrkHbEJ requirements.txt\n",
            "Processing file 1q_RyS877-ekSdE7pXWsdSBz9xaHzqAvZ test_train_logreg.py\n",
            "Processing file 1LtsEQr4j9z6W26iVg-JL1Gk2zaQFT91J test.py\n",
            "Processing file 17Q7q29LfqA45rEgB7o1iWm-PTWINGl6y train.py\n",
            "Processing file 1HdyKsKlZUeW848Po6BU_HsL7tyIABV1t train_model.sh\n",
            "Building directory structure completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=19fs8nuUI0ZHWoVQRrs3L6h5cBT2SFTOv\n",
            "From (redirected): https://drive.google.com/uc?id=19fs8nuUI0ZHWoVQRrs3L6h5cBT2SFTOv&confirm=t&uuid=6840ce05-90c6-47b5-aa67-dc6d3a7335e1\n",
            "To: /content/CS598-DL4H-Team18-final/data/data_arrays.npz\n",
            "100%|██████████| 436M/436M [00:02<00:00, 170MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qcrNWb1GdBX8jct_qZaLBIkgVR3MdIz7\n",
            "To: /content/CS598-DL4H-Team18-final/data/test_ids_patients.pkl\n",
            "100%|██████████| 18.9k/18.9k [00:00<00:00, 27.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kIXfybocTDIhxRsXfshISOyVHrUjMmPF\n",
            "To: /content/CS598-DL4H-Team18-final/Dockerfile\n",
            "100%|██████████| 219/219 [00:00<00:00, 524kB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ZtodqJxf2eb8aJsENZEwgg2PtPvr23wi\n",
            "From (redirected): https://drive.google.com/uc?id=1ZtodqJxf2eb8aJsENZEwgg2PtPvr23wi&confirm=t&uuid=d4fd867f-fe0f-454e-9039-fb88be8ae2b7\n",
            "To: /content/CS598-DL4H-Team18-final/eval_model.sh\n",
            "100%|██████████| 627/627 [00:00<00:00, 488kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pRvui7NzpRqiJnP2sx2ued0SueM0cS3i\n",
            "To: /content/CS598-DL4H-Team18-final/logdir/ode_birnn_attention/epoch_times.npz\n",
            "100%|██████████| 916/916 [00:00<00:00, 585kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pruZTvX5h-jvHBJb8jZzEb-HCCXCLYJH\n",
            "To: /content/CS598-DL4H-Team18-final/logdir/ode_birnn_attention/final_model.pt\n",
            "100%|██████████| 71.1k/71.1k [00:00<00:00, 56.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dqumtKiLMBR2444NuI2wx650HRS3IHON\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/ADMISSIONS.csv\n",
            "100%|██████████| 24.9k/24.9k [00:00<00:00, 32.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sqOu39urse4dqRLJdfg7Nwfh9XQYe3_U\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/CALLOUT.csv\n",
            "100%|██████████| 12.5k/12.5k [00:00<00:00, 16.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dI3tHD_LcfdDi0xxVWzK2koBkMMvM-qP\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/CAREGIVERS.csv\n",
            "100%|██████████| 186k/186k [00:00<00:00, 23.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wHQAiBfRw4w0hi_8O8mEGis-ApV0o63B\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/CHARTEVENTS.csv\n",
            "100%|██████████| 71.6M/71.6M [00:00<00:00, 174MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eETouFkHzGO3c1cKn3Z_MNUVSVe8fYIz\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/D_ITEMS.csv\n",
            "100%|██████████| 866k/866k [00:00<00:00, 31.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GkmohzxqOvsMdUuzgsndwc_nkTQhqLMU\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/DIAGNOSES_ICD.csv\n",
            "100%|██████████| 50.7k/50.7k [00:00<00:00, 60.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xUzZVlwCbiT5aRbGhZwIWAIPc7QP9axy\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/ICUSTAYS.csv\n",
            "100%|██████████| 12.5k/12.5k [00:00<00:00, 17.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Nne2jTU4VbryveHfAAi6LZlCCqDzwcuW\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/OUTPUTEVENTS.csv\n",
            "100%|██████████| 925k/925k [00:00<00:00, 30.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f6iBzMwPcHIxXQoc2j6hGChAqjoW62oq\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/PATIENTS.csv\n",
            "100%|██████████| 7.00k/7.00k [00:00<00:00, 13.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=134SvY8X-Odf7l13UuDPyxOrBKagyWEPP\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/PRESCRIPTIONS.csv\n",
            "100%|██████████| 1.55M/1.55M [00:00<00:00, 59.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1g9V9pZBszszh6lHwyN2_NUujW0BIOxil\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/PROCEDURES_ICD.csv\n",
            "100%|██████████| 14.0k/14.0k [00:00<00:00, 19.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AJb7I1FOWzO_RPY7HFK5J930Qxrfmf4F\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/SERVICES.csv\n",
            "100%|██████████| 6.93k/6.93k [00:00<00:00, 6.01MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BsKTKVzov79ZXmQf-8JmWQTEIN5zwQEy\n",
            "To: /content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/SHA256SUMS.txt\n",
            "100%|██████████| 2.19k/2.19k [00:00<00:00, 4.71MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1J-g2RCsI-amIj95YEDFOHbSXp-bDBMLF\n",
            "To: /content/CS598-DL4H-Team18-final/README.md\n",
            "100%|██████████| 19.0/19.0 [00:00<00:00, 43.3kB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Y1uXRPxLIuYCGfaQFBWf3J9h68F6eUA_\n",
            "From (redirected): https://drive.google.com/uc?id=1Y1uXRPxLIuYCGfaQFBWf3J9h68F6eUA_&confirm=t&uuid=a07292ae-0c09-49c4-837b-26de27b7f846\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/1_preprocessing_ICU_PAT_ADMIT.py\n",
            "100%|██████████| 11.1k/11.1k [00:00<00:00, 16.7MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1HaKx7rV4PXb8Y4IqiBZlrbNc5TqTDlWy\n",
            "From (redirected): https://drive.google.com/uc?id=1HaKx7rV4PXb8Y4IqiBZlrbNc5TqTDlWy&confirm=t&uuid=c3ff477f-f23d-49ca-8de3-cbe50f303257\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/2_preprocessing_reduce_charts.py\n",
            "100%|██████████| 6.88k/6.88k [00:00<00:00, 4.25MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1WOqdUwJHZHs-8mmMtD6ozQQ63rc6H0RO\n",
            "From (redirected): https://drive.google.com/uc?id=1WOqdUwJHZHs-8mmMtD6ozQQ63rc6H0RO&confirm=t&uuid=77b4e9eb-8056-49f6-a2dd-5c0b25d26852\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/3_preprocessing_reduce_outputs.py\n",
            "100%|██████████| 4.46k/4.46k [00:00<00:00, 7.05MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1tX_-bkIcUY6p_mfZs0KBxYc6t7qDTH6D\n",
            "From (redirected): https://drive.google.com/uc?id=1tX_-bkIcUY6p_mfZs0KBxYc6t7qDTH6D&confirm=t&uuid=d520b067-ed2b-427d-9e45-bc432a52b7ae\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/4_preprocessing_merge_charts_outputs.py\n",
            "100%|██████████| 7.69k/7.69k [00:00<00:00, 14.5MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1DPDi7dJNxxoxYDpfn-B4EcWRfCRqyvTN\n",
            "From (redirected): https://drive.google.com/uc?id=1DPDi7dJNxxoxYDpfn-B4EcWRfCRqyvTN&confirm=t&uuid=c93aae2d-0bb4-4603-9b7b-13101ae878ff\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/5_preprocessing_CHARTS_PRESCRIPTIONS.py\n",
            "100%|██████████| 2.51k/2.51k [00:00<00:00, 5.76MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1oMrKKWoEUb9WLwNaQ6Ktctl9kDsMxfhN\n",
            "From (redirected): https://drive.google.com/uc?id=1oMrKKWoEUb9WLwNaQ6Ktctl9kDsMxfhN&confirm=t&uuid=0fc09673-19ba-43a0-be03-3946fa642abf\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/6_preprocessing_DIAGNOSES_PROCEDURES.py\n",
            "100%|██████████| 3.73k/3.73k [00:00<00:00, 7.90MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ZjUIW8PDhDSj0I14dgJhuIrIXCKUhRxB\n",
            "From (redirected): https://drive.google.com/uc?id=1ZjUIW8PDhDSj0I14dgJhuIrIXCKUhRxB&confirm=t&uuid=9a0e20c8-ca9f-42b9-aae3-5ea1e6839477\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/7_preprocessing_create_arrays.py\n",
            "100%|██████████| 3.91k/3.91k [00:00<00:00, 4.90MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1d8rj1Tba70QGozvC9GZB0-hI0qJf1hOB\n",
            "From (redirected): https://drive.google.com/uc?id=1d8rj1Tba70QGozvC9GZB0-hI0qJf1hOB&confirm=t&uuid=07eeb929-66c2-4206-a9da-96d6611f4798\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/bayesian_interpretation.py\n",
            "100%|██████████| 5.97k/5.97k [00:00<00:00, 10.4MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1hSFETY2OhJgdCXY1mJx1j-B2CEzmxZuR\n",
            "From (redirected): https://drive.google.com/uc?id=1hSFETY2OhJgdCXY1mJx1j-B2CEzmxZuR&confirm=t&uuid=97363ae7-f79c-432e-bb0a-d00e1c9302a0\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/bayesian_save_embeddings.py\n",
            "100%|██████████| 2.26k/2.26k [00:00<00:00, 4.64MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1o3AO9diTyksNfu_Jwelw5x2rncPcDWOU\n",
            "From (redirected): https://drive.google.com/uc?id=1o3AO9diTyksNfu_Jwelw5x2rncPcDWOU&confirm=t&uuid=7736d0e0-4acf-4f77-a2a3-11e3ea002e36\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/bayesian_train.py\n",
            "100%|██████████| 12.2k/12.2k [00:00<00:00, 19.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Ibk2slcMzdDk9TyRjmJsby-6jAHj9CQq\n",
            "From (redirected): https://drive.google.com/uc?id=1Ibk2slcMzdDk9TyRjmJsby-6jAHj9CQq&confirm=t&uuid=d1be20ac-b413-4423-b4cc-6b62d0832aa6\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/data_load.py\n",
            "100%|██████████| 2.93k/2.93k [00:00<00:00, 6.13MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17SY9JyMy4UFEIEUGa5AaRh8-rEBNGuus\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/embeddings/emb_weight_cp_6.npy\n",
            "100%|██████████| 29.9k/29.9k [00:00<00:00, 42.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1a60NvUXpScCuI9joqXUaf2eCTnQjpk-3\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/embeddings/emb_weight_cp_11.npy\n",
            "100%|██████████| 54.7k/54.7k [00:00<00:00, 29.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1W1L2w_Zbq3Q2zJk1LQmMeWqmrrfvG5Ay\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/embeddings/emb_weight_dp_7.npy\n",
            "100%|██████████| 72.5k/72.5k [00:00<00:00, 48.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oB3Cd2ZSj4iQ858HXxpUSIGc_sBdnCm-\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/embeddings/emb_weight_dp_13.npy\n",
            "100%|██████████| 134k/134k [00:00<00:00, 52.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=13zm26rDBWhyCYov2mnM4dEBtISY-5HLv\n",
            "From (redirected): https://drive.google.com/uc?id=13zm26rDBWhyCYov2mnM4dEBtISY-5HLv&confirm=t&uuid=7eb1a43a-df53-4ac7-8e6a-acb9dbf97c97\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/helper_eval_model.py\n",
            "100%|██████████| 1.33k/1.33k [00:00<00:00, 3.76MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_Km0OmLnqbofBnzIXZoc8cSLFPwNMv5L\n",
            "From (redirected): https://drive.google.com/uc?id=1_Km0OmLnqbofBnzIXZoc8cSLFPwNMv5L&confirm=t&uuid=c4979e94-4dbb-47e0-b1e1-973ebdeb0e32\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/helper_train_model.py\n",
            "100%|██████████| 1.33k/1.33k [00:00<00:00, 3.05MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1EfXKHvamwSV2R4OiTBApkX9xIn9MStCD\n",
            "From (redirected): https://drive.google.com/uc?id=1EfXKHvamwSV2R4OiTBApkX9xIn9MStCD&confirm=t&uuid=8fcb87fe-0dc7-47f4-88fd-1d12ed66b573\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/hyperparameters.py\n",
            "100%|██████████| 1.26k/1.26k [00:00<00:00, 2.78MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1NnVzdM0HyUuEpvJy2XsO2K1blMJbFFa5\n",
            "From (redirected): https://drive.google.com/uc?id=1NnVzdM0HyUuEpvJy2XsO2K1blMJbFFa5&confirm=t&uuid=93934dae-c944-4aa6-b7db-b20fc4c61f39\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/modules_ode.py\n",
            "100%|██████████| 7.30k/7.30k [00:00<00:00, 10.5MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1KkEIepYUkfhkPkSgxqGvPdzvhIdolt0A\n",
            "From (redirected): https://drive.google.com/uc?id=1KkEIepYUkfhkPkSgxqGvPdzvhIdolt0A&confirm=t&uuid=d63f033f-6d3a-4970-b7cc-cae6ea6bada0\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/modules.py\n",
            "100%|██████████| 51.4k/51.4k [00:00<00:00, 16.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15Z_1uOxkB7hF-zPPkc5EaoWIitrkHbEJ\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/requirements.txt\n",
            "100%|██████████| 372/372 [00:00<00:00, 428kB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1q_RyS877-ekSdE7pXWsdSBz9xaHzqAvZ\n",
            "From (redirected): https://drive.google.com/uc?id=1q_RyS877-ekSdE7pXWsdSBz9xaHzqAvZ&confirm=t&uuid=56801273-651e-4c86-87e9-d25973bf31da\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/test_train_logreg.py\n",
            "100%|██████████| 7.14k/7.14k [00:00<00:00, 15.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1LtsEQr4j9z6W26iVg-JL1Gk2zaQFT91J\n",
            "From (redirected): https://drive.google.com/uc?id=1LtsEQr4j9z6W26iVg-JL1Gk2zaQFT91J&confirm=t&uuid=cc81f250-d854-4ad3-92bd-0eb156452008\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/test.py\n",
            "100%|██████████| 6.22k/6.22k [00:00<00:00, 10.1MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=17Q7q29LfqA45rEgB7o1iWm-PTWINGl6y\n",
            "From (redirected): https://drive.google.com/uc?id=17Q7q29LfqA45rEgB7o1iWm-PTWINGl6y&confirm=t&uuid=a606900c-9d6f-4027-a6de-a0956573b566\n",
            "To: /content/CS598-DL4H-Team18-final/related_code/train.py\n",
            "100%|██████████| 2.56k/2.56k [00:00<00:00, 6.95MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1HdyKsKlZUeW848Po6BU_HsL7tyIABV1t\n",
            "From (redirected): https://drive.google.com/uc?id=1HdyKsKlZUeW848Po6BU_HsL7tyIABV1t&confirm=t&uuid=bb3d6416-7232-4bed-af8e-6f66684c0b42\n",
            "To: /content/CS598-DL4H-Team18-final/train_model.sh\n",
            "100%|██████████| 421/421 [00:00<00:00, 1.07MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/CS598-DL4H-Team18-final/data/data_arrays.npz',\n",
              " '/content/CS598-DL4H-Team18-final/data/test_ids_patients.pkl',\n",
              " '/content/CS598-DL4H-Team18-final/Dockerfile',\n",
              " '/content/CS598-DL4H-Team18-final/eval_model.sh',\n",
              " '/content/CS598-DL4H-Team18-final/logdir/ode_birnn_attention/epoch_times.npz',\n",
              " '/content/CS598-DL4H-Team18-final/logdir/ode_birnn_attention/final_model.pt',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/ADMISSIONS.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/CALLOUT.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/CAREGIVERS.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/CHARTEVENTS.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/D_ITEMS.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/DIAGNOSES_ICD.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/ICUSTAYS.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/OUTPUTEVENTS.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/PATIENTS.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/PRESCRIPTIONS.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/PROCEDURES_ICD.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/SERVICES.csv',\n",
              " '/content/CS598-DL4H-Team18-final/mimic-iii-clinical-database-1.4/SHA256SUMS.txt',\n",
              " '/content/CS598-DL4H-Team18-final/README.md',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/1_preprocessing_ICU_PAT_ADMIT.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/2_preprocessing_reduce_charts.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/3_preprocessing_reduce_outputs.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/4_preprocessing_merge_charts_outputs.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/5_preprocessing_CHARTS_PRESCRIPTIONS.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/6_preprocessing_DIAGNOSES_PROCEDURES.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/7_preprocessing_create_arrays.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/bayesian_interpretation.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/bayesian_save_embeddings.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/bayesian_train.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/data_load.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/embeddings/emb_weight_cp_6.npy',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/embeddings/emb_weight_cp_11.npy',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/embeddings/emb_weight_dp_7.npy',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/embeddings/emb_weight_dp_13.npy',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/helper_eval_model.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/helper_train_model.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/hyperparameters.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/modules_ode.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/modules.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/requirements.txt',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/test_train_logreg.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/test.py',\n",
              " '/content/CS598-DL4H-Team18-final/related_code/train.py',\n",
              " '/content/CS598-DL4H-Team18-final/train_model.sh']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the trained model (done on full version of MIMIC-III dataset on local environment) by executing the following:"
      ],
      "metadata": {
        "id": "N3XimPo8ackB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gBdVZoTvsSFV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e3b9042c-c313-4e04-8601-32c0ca998114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CS598-DL4H-Team18-final\n"
          ]
        }
      ],
      "source": [
        "%cd /content/CS598-DL4H-Team18-final\n",
        "# Load the model from the .pt file\n",
        "model = torch.load('/content/CS598-DL4H-Team18-final/logdir/ode_birnn_attention/final_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shell script (training.sh) calls a helper function which call the below method for training. This is a code snippet of the training model that we are running:"
      ],
      "metadata": {
        "id": "B20Uz-Bcuywo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #If there is any issue related to libraries installed, please uncomment the following command and execute it:\n",
        "# #!pip install -r /content/CS598DL4H_Team_18/related_code/requirements.txt\n",
        "\n",
        "# # Code snippet for training:\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# import numpy as np\n",
        "# from hyperparameters import Hyperparameters as hp\n",
        "# from data_load import *\n",
        "# from modules import *\n",
        "# import os\n",
        "# from tqdm import tqdm\n",
        "# from time import time\n",
        "# from sklearn.metrics import accuracy_score, confusion_matrix, average_precision_score, roc_auc_score, f1_score\n",
        "# from pdb import set_trace as bp\n",
        "\n",
        "\n",
        "# def main():\n",
        "#   # Load data\n",
        "#   print('Load data...')\n",
        "#   data = np.load(hp.data_dir + 'data_arrays.npz', allow_pickle=True)\n",
        "\n",
        "#   # Training and validation data\n",
        "#   if hp.all_train:\n",
        "#     trainloader, num_batches, pos_weight = get_trainloader(data, 'ALL')\n",
        "#   else:\n",
        "#     trainloader, num_batches, pos_weight = get_trainloader(data, 'TRAIN')\n",
        "\n",
        "#   # Vocabulary sizes\n",
        "#   num_static = num_static_data(data)\n",
        "#   num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
        "\n",
        "#   print('-----------------------------------------')\n",
        "#   print('Train...')\n",
        "\n",
        "#   # CUDA for PyTorch\n",
        "#   use_cuda = torch.cuda.is_available()\n",
        "#   device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "#   torch.backends.cudnn.benchmark = True\n",
        "\n",
        "#   # Network\n",
        "#   net = Net(num_static, num_dp_codes, num_cp_codes).to(device)\n",
        "\n",
        "#   # Loss function and optimizer\n",
        "#   criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight).to(device)\n",
        "#   optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
        "\n",
        "#   # Create log dir\n",
        "#   logdir = hp.logdir + hp.net_variant + '/'\n",
        "#   if not os.path.exists(logdir):\n",
        "#     os.makedirs(logdir)\n",
        "\n",
        "#   # Store times\n",
        "#   epoch_times = []\n",
        "\n",
        "#   # Train\n",
        "#   for epoch in tqdm(range(hp.num_epochs)):\n",
        "#     # print('-----------------------------------------')\n",
        "#     # print('Epoch: {}'.format(epoch))\n",
        "#     net.train()\n",
        "#     time_start = time()\n",
        "#     for i, (stat, dp, cp, dp_t, cp_t, label) in enumerate(tqdm(trainloader), 0):\n",
        "#       # move to GPU if available\n",
        "#       stat  = stat.to(device)\n",
        "#       dp    = dp.to(device)\n",
        "#       cp    = cp.to(device)\n",
        "#       dp_t  = dp_t.to(device)\n",
        "#       cp_t  = cp_t.to(device)\n",
        "#       label = label.to(device)\n",
        "\n",
        "#       # zero the parameter gradients\n",
        "#       optimizer.zero_grad()\n",
        "\n",
        "#       # forward + backward + optimize\n",
        "#       label_pred, _ = net(stat, dp, cp, dp_t, cp_t)\n",
        "#       loss = criterion(label_pred, label)\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "\n",
        "#     # timing\n",
        "#     time_end = time()\n",
        "#     epoch_times.append(time_end-time_start)\n",
        "\n",
        "#   # Save\n",
        "#   print('Saving...')\n",
        "#   torch.save(net.state_dict(), logdir + 'final_model.pt')\n",
        "#   np.savez(logdir + 'epoch_times', epoch_times=epoch_times)\n",
        "#   print('Done')\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   main()"
      ],
      "metadata": {
        "id": "x9asR0p-tEKJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "We will run only 5 iterations in this notebook for demonstration. For reproducing the original paper, we have run 100 samples on our local setup.\n",
        "\n",
        "These are performance metrics used to evaluate the ODE + RNN + Attention model described in the paper. Here's what each of these metrics represents:\n",
        "\n",
        "* **Average Precision:** Average Precision is a metric used to evaluate the quality of binary classification models. It computes the area under the precision-recall curve, which represents the trade-off between precision and recall. Higher values indicate better performance.\n",
        "\n",
        "* **AUROC (Area Under the Receiver Operating Characteristic Curve):** AUROC is another metric used to evaluate binary classification models. It measures the ability of the model to distinguish between positive and negative classes across different threshold values. Higher values indicate better discrimination performance.\n",
        "\n",
        "* **F1 Score:** F1 Score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall, making it useful for imbalanced datasets or when there is an uneven cost associated with false positives and false negatives.\n",
        "\n",
        "* **PPV (Positive Predictive Value):** PPV, also known as precision, measures the proportion of true positive predictions out of all positive predictions made by the model. It represents the model's ability to correctly identify positive instances.\n",
        "\n",
        "* **NPV (Negative Predictive Value):** NPV measures the proportion of true negative predictions out of all negative predictions made by the model. It represents the model's ability to correctly identify negative instances.\n",
        "\n",
        "* **Sensitivity (True Positive Rate):** Sensitivity, also known as recall or true positive rate, measures the proportion of true positive predictions out of all actual positive instances. It represents the model's ability to correctly identify positive instances among all positive instances in the dataset.\n",
        "\n",
        "* **Specificity (True Negative Rate):** Specificity measures the proportion of true negative predictions out of all actual negative instances. It represents the model's ability to correctly identify negative instances among all negative instances in the dataset.\n",
        "\n",
        "* **Time:** Time represents the runtime of the model, typically measured in seconds. The values provided (mean and range) indicate the average runtime and its variability across different runs or experiments."
      ],
      "metadata": {
        "id": "e7UnbP7lY7sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please note:** The evaluation here is being done on the trained model which was trained using the full version of the MIMIC-III dataset in order to show the actual results. Hence, as the dataset is extremely large it would take some time."
      ],
      "metadata": {
        "id": "4Y7BM10R4nAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Type 1 in the input field. Ablations are in progress.\n",
        "!bash /content/CS598-DL4H-Team18-final/eval_model.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Lwk3gPgAoUHa",
        "outputId": "6eecbb48-c1fb-4587-f8e9-b6a9337dd1c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) ODE + RNN + Attention       3) *ABLATION* RNN\n",
            "2) *ABLATION* RNN + Attention  4) QUIT\n",
            "Which model do you want to evaluate? 1\n",
            "====================================\n",
            "Evaluating ODE + RNN + Attention (ode_birnn_attention) ...\n",
            "====================================\n",
            "Load data...\n",
            "Bootstrap sample 0\n",
            "  0% 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Euler: Unexpected arguments {'max_num_steps': 1000}\n",
            "  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))\n",
            "100% 35/35 [00:33<00:00,  1.06it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Bootstrap sample 1\n",
            "  0% 0/36 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Euler: Unexpected arguments {'max_num_steps': 1000}\n",
            "  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))\n",
            "100% 36/36 [00:35<00:00,  1.01it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Bootstrap sample 2\n",
            "  0% 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Euler: Unexpected arguments {'max_num_steps': 1000}\n",
            "  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))\n",
            "100% 35/35 [00:30<00:00,  1.15it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Bootstrap sample 3\n",
            "  0% 0/36 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Euler: Unexpected arguments {'max_num_steps': 1000}\n",
            "  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))\n",
            "100% 36/36 [00:35<00:00,  1.02it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Bootstrap sample 4\n",
            "  0% 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchdiffeq/_impl/misc.py:11: UserWarning: Euler: Unexpected arguments {'max_num_steps': 1000}\n",
            "  warnings.warn('{}: Unexpected arguments {}'.format(solver.__class__.__name__, unused_kwargs))\n",
            "100% 35/35 [00:31<00:00,  1.11it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2241: RuntimeWarning: invalid value encountered in multiply\n",
            "  lower_bound = _a * scale + loc\n",
            "/usr/local/lib/python3.10/dist-packages/scipy/stats/_distn_infrastructure.py:2242: RuntimeWarning: invalid value encountered in multiply\n",
            "  upper_bound = _b * scale + loc\n",
            "------------------------------------------------\n",
            "Net variant: ode_birnn_attention\n",
            "Average Precision: 0.316 [0.278,0.354]\n",
            "AUROC: 0.729 [0.716,0.742]\n",
            "F1: 0.364 [0.335,0.393]\n",
            "PPV: 1.0 [nan,nan]\n",
            "NPV: 0.879 [0.872,0.887]\n",
            "Sensitivity: 0.717 [0.68,0.753]\n",
            "Specificity: 0.646 [0.624,0.669]\n",
            "Time: 787.761 [450.875,1124.647] std: 1504.338\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the code provided by the original authors for test and validation. Please refer to https://github.com/aprakash16/CS598DL4H_Team_18/blob/main/related_code/test.py for the *test.py* code."
      ],
      "metadata": {
        "id": "FXMOqemcg14A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shell script (eval.sh) calls a helper function which call the below method for evaluation. This is a code snippet of the test.py that we are running:"
      ],
      "metadata": {
        "id": "7d5d67xtdjP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from __future__ import print_function\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import pickle\n",
        "# import scipy.stats as st\n",
        "# from hyperparameters import Hyperparameters as hp\n",
        "# from data_load import *\n",
        "# from modules import *\n",
        "# import os\n",
        "# from tqdm import tqdm\n",
        "# from train import Net\n",
        "# #import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import *\n",
        "# from sklearn.calibration import calibration_curve\n",
        "# from pdb import set_trace as bp\n",
        "\n",
        "# def round(num):\n",
        "#   return np.round(num*1000)/1000\n",
        "\n",
        "# def main():\n",
        "#   # Load data\n",
        "#   print('Load data...')\n",
        "#   data = np.load(hp.data_dir + 'data_arrays.npz')\n",
        "#   test_ids_patients = pd.read_pickle(hp.data_dir + 'test_ids_patients.pkl')\n",
        "\n",
        "#   # Patients in test data\n",
        "#   patients = test_ids_patients.drop_duplicates()\n",
        "#   num_patients = patients.shape[0]\n",
        "#   row_ids = pd.DataFrame({'ROW_IDX': test_ids_patients.index}, index=test_ids_patients)\n",
        "\n",
        "#   # Vocabulary sizes\n",
        "#   num_static = num_static_data(data)\n",
        "#   num_dp_codes, num_cp_codes = vocab_sizes(data)\n",
        "\n",
        "#   # CUDA for PyTorch\n",
        "#   use_cuda = torch.cuda.is_available()\n",
        "#   device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
        "#   torch.backends.cudnn.benchmark = True\n",
        "\n",
        "#   # Network\n",
        "#   net = Net(num_static, num_dp_codes, num_cp_codes).to(device)\n",
        "\n",
        "#   # Set log dir to read trained model from\n",
        "#   logdir = hp.logdir + hp.net_variant + '/'\n",
        "\n",
        "#   # Restore variables from disk\n",
        "#   net.load_state_dict(torch.load(logdir + 'final_model.pt', map_location=device))\n",
        "\n",
        "#   # Bootstrapping\n",
        "#   np.random.seed(hp.np_seed)\n",
        "#   avpre_vec = np.zeros(hp.bootstrap_samples)\n",
        "#   auroc_vec = np.zeros(hp.bootstrap_samples)\n",
        "#   f1_vec    = np.zeros(hp.bootstrap_samples)\n",
        "#   sensitivity_vec = np.zeros(hp.bootstrap_samples)\n",
        "#   specificity_vec = np.zeros(hp.bootstrap_samples)\n",
        "#   ppv_vec = np.zeros(hp.bootstrap_samples)\n",
        "#   npv_vec = np.zeros(hp.bootstrap_samples)\n",
        "\n",
        "#   for sample in range(hp.bootstrap_samples):\n",
        "#     print('Bootstrap sample {}'.format(sample))\n",
        "\n",
        "#     # Test data\n",
        "#     sample_patients = patients.sample(n=num_patients, replace=True)\n",
        "#     idx = np.squeeze(row_ids.loc[sample_patients].values)\n",
        "#     testloader, _, _ = get_trainloader(data, 'TEST', shuffle=False, idx=idx)\n",
        "\n",
        "#     # evaluate on test data\n",
        "#     net.eval()\n",
        "#     label_pred = torch.Tensor([])\n",
        "#     label_test = torch.Tensor([])\n",
        "#     with torch.no_grad():\n",
        "#       for i, (stat, dp, cp, dp_t, cp_t, label_batch) in enumerate(tqdm(testloader), 0):\n",
        "#         # move to GPU if available\n",
        "#         stat  = stat.to(device)\n",
        "#         dp    = dp.to(device)\n",
        "#         cp    = cp.to(device)\n",
        "#         dp_t  = dp_t.to(device)\n",
        "#         cp_t  = cp_t.to(device)\n",
        "\n",
        "#         label_pred_batch, _ = net(stat, dp, cp, dp_t, cp_t)\n",
        "#         label_pred = torch.cat((label_pred, label_pred_batch.cpu()))\n",
        "#         label_test = torch.cat((label_test, label_batch))\n",
        "\n",
        "#     label_sigmoids = torch.sigmoid(label_pred).cpu().numpy()\n",
        "\n",
        "#     # Average precision\n",
        "#     avpre = average_precision_score(label_test, label_sigmoids)\n",
        "\n",
        "#     # Determine AUROC score\n",
        "#     auroc = roc_auc_score(label_test, label_sigmoids)\n",
        "\n",
        "#     # Sensitivity, specificity\n",
        "#     fpr, tpr, thresholds = roc_curve(label_test, label_sigmoids)\n",
        "#     youden_idx = np.argmax(tpr - fpr)\n",
        "#     sensitivity = tpr[youden_idx]\n",
        "#     specificity = 1-fpr[youden_idx]\n",
        "\n",
        "#     # F1, PPV, NPV score\n",
        "#     f1 = 0\n",
        "#     ppv = 0\n",
        "#     npv = 0\n",
        "#     for t in thresholds:\n",
        "#       label_pred = (np.array(label_sigmoids) >= t).astype(int)\n",
        "#       f1_temp = f1_score(label_test, label_pred)\n",
        "#       ppv_temp = precision_score(label_test, label_pred, pos_label=1)\n",
        "#       npv_temp = precision_score(label_test, label_pred, pos_label=0)\n",
        "#       if f1_temp > f1:\n",
        "#         f1 = f1_temp\n",
        "#       if (ppv_temp+npv_temp) > (ppv+npv):\n",
        "#         ppv = ppv_temp\n",
        "#         npv = npv_temp\n",
        "\n",
        "#     # Store in vectors\n",
        "#     avpre_vec[sample] = avpre\n",
        "#     auroc_vec[sample] = auroc\n",
        "#     f1_vec[sample]    = f1\n",
        "#     sensitivity_vec[sample]  = sensitivity\n",
        "#     specificity_vec[sample]  = specificity\n",
        "#     ppv_vec[sample]  = ppv\n",
        "#     npv_vec[sample]  = npv\n",
        "\n",
        "#   avpre_mean = np.mean(avpre_vec)\n",
        "#   avpre_lci, avpre_uci = st.t.interval(0.95, hp.bootstrap_samples-1, loc=avpre_mean, scale=st.sem(avpre_vec))\n",
        "#   auroc_mean = np.mean(auroc_vec)\n",
        "#   auroc_lci, auroc_uci = st.t.interval(0.95, hp.bootstrap_samples-1, loc=auroc_mean, scale=st.sem(auroc_vec))\n",
        "#   f1_mean = np.mean(f1_vec)\n",
        "#   f1_lci, f1_uci = st.t.interval(0.95, hp.bootstrap_samples-1, loc=f1_mean, scale=st.sem(f1_vec))\n",
        "#   ppv_mean = np.mean(ppv_vec)\n",
        "#   ppv_lci, ppv_uci = st.t.interval(0.95, hp.bootstrap_samples-1, loc=ppv_mean, scale=st.sem(ppv_vec))\n",
        "#   npv_mean = np.mean(npv_vec)\n",
        "#   npv_lci, npv_uci = st.t.interval(0.95, hp.bootstrap_samples-1, loc=npv_mean, scale=st.sem(npv_vec))\n",
        "#   sensitivity_mean = np.mean(sensitivity_vec)\n",
        "#   sensitivity_lci, sensitivity_uci = st.t.interval(0.95, hp.bootstrap_samples-1, loc=sensitivity_mean, scale=st.sem(sensitivity_vec))\n",
        "#   specificity_mean = np.mean(specificity_vec)\n",
        "#   specificity_lci, specificity_uci = st.t.interval(0.95, hp.bootstrap_samples-1, loc=specificity_mean, scale=st.sem(specificity_vec))\n",
        "\n",
        "#   epoch_times = np.load(hp.logdir + hp.net_variant + '/epoch_times.npz')['epoch_times']\n",
        "#   times_mean = np.mean(epoch_times)\n",
        "#   times_lci, times_uci = st.t.interval(0.95, len(epoch_times)-1, loc=np.mean(epoch_times), scale=st.sem(epoch_times))\n",
        "#   times_std = np.std(epoch_times)\n",
        "\n",
        "#   print('------------------------------------------------')\n",
        "#   print('Net variant: {}'.format(hp.net_variant))\n",
        "#   print('Average Precision: {} [{},{}]'.format(round(avpre_mean), round(avpre_lci), round(avpre_uci)))\n",
        "#   print('AUROC: {} [{},{}]'.format(round(auroc_mean), round(auroc_lci), round(auroc_uci)))\n",
        "#   print('F1: {} [{},{}]'.format(round(f1_mean), round(f1_lci), round(f1_uci)))\n",
        "#   print('PPV: {} [{},{}]'.format(round(ppv_mean), round(ppv_lci), round(ppv_uci)))\n",
        "#   print('NPV: {} [{},{}]'.format(round(npv_mean), round(npv_lci), round(npv_uci)))\n",
        "#   print('Sensitivity: {} [{},{}]'.format(round(sensitivity_mean), round(sensitivity_lci), round(sensitivity_uci)))\n",
        "#   print('Specificity: {} [{},{}]'.format(round(specificity_mean), round(specificity_lci), round(specificity_uci)))\n",
        "#   print('Time: {} [{},{}] std: {}'.format(round(times_mean), round(times_lci), round(times_uci), round(times_std)))\n",
        "#   print('Done')\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   main()\n"
      ],
      "metadata": {
        "id": "5yVl4PxBj--4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result\n",
        "We were able to reproduce the ODE+RNN+Attention model (using full version of MIMIC-III dataset) with the following results on local setup:\n",
        "\n",
        "Average Precision: 0.311 [0.304,0.319]\n",
        "AUROC (Area Under the Receiver Operating Characteristic Curve): 0.74 [0.738,0.743]\n",
        "F1 Score: 0.363 [0.358,0.368]\n",
        "PPV (Positive Predictive Value): 0.983 [0.967,1.0]\n",
        "NPV (Negative Predictive Value): 0.882 [0.881,0.884]\n",
        "Sensitivity: 0.719 [0.711,0.728]\n",
        "Specificity: 0.66 [0.652,0.669]"
      ],
      "metadata": {
        "id": "jFSLVKArx3vK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Analysis and Model comparison\n",
        "\n",
        "Referring to metrics given in the original paper, the following results were shown:\n",
        "\n",
        "**RNN (ODE time decay) + Attention:**\n",
        "  * **Average Precision:** 0.316 <br>\n",
        "  * **AUROC (Area Under the Receiver Operating Characteristic Curve):** 0.743<br>\n",
        "  * **F1 Score:** 0.375<br>\n",
        "  * **Sensitivity:** 0.648<br>\n",
        "  * **Specificity:** 0.733<br>\n",
        "\n",
        "**ODE + RNN:**\n",
        "  * **Average Precision:** 0.331 <br>\n",
        "  * **AUROC (Area Under the Receiver Operating Characteristic Curve):** 0.739<br>\n",
        "  * **F1 Score:** 0.372<br>\n",
        "  * **Sensitivity:** 0.672<br>\n",
        "  * **Specificity:** 0.697<br>\n",
        "\n",
        "Based on these metrics, we can observe the following:\n",
        "\n",
        "Hypothesis 1: The ODE + RNN + Attention model performs slightly worse in terms of Average Precision compared to the ODE + RNN model (Average Precision: 0.311 vs. 0.331). This supports the hypothesis that recurrent models may outperform models with only attention layers by a small margin.\n",
        "\n",
        "Hypothesis 2: Both models ODE + RNN + Attention and RNN (ODE time decay) + Attention have comparable Average Precision scores while incorporating attention layers. This suggests that adding attention layers does not degrade the average precision of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan\n",
        "The future plan is to work on the ablations proposed that have been planned in order to observe the impact of eliminatinf timestamped code from model's embeddings. Also, we will work on the video presentatione and xplain what the original paper is about (what the general problem is, what the specific approach was taken, and what results were claimed). It will also have what we encountered when we attempted to reproduce the results."
      ],
      "metadata": {
        "id": "HPTVFTzqy9ir"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "\n",
        "After configuring the environment, implementing the original authors' code was straightforward.\n",
        "The original authors' code was well-written, making it straightforward to comprehend and navigate.\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not. - The steps were clear and the file was well-written. Hence, the paper was reproducible\n",
        "  * Describe “What was easy” and “What was difficult” during the reproduction.\n",
        "    - What was easy?: After configuring the environment, implementing the original authors' code was straightforward. The original authors' code was well-written, making it straightforward to comprehend and navigate.\n",
        "    - What was difficult?: The exact Python version was not mentioned. Hence, few libraries were incompatible with the current version and we had to degrade Python version in docker. Also, the training took approximately 10 hrs on local setup. It was very time consuming.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility. - Use the provided Dockerfile for isolated setup. Running on local system is a tedious task especiallhy if the versions are incompatible. Also, mention the system on system details where the model was trained and tested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Barbieri, Sebastiano, et al. “Benchmarking deep learning architectures for predicting readmission to the ICU and describing patients-at-risk,” Scientific reports 10.1, 2020, 1-10, https://doi.org/10.1038/s41598-020-58053-z\n",
        "\n",
        "2. Johnson, A., Pollard, T., Shen, L. et al. “MIMIC-III, a freely accessible critical care database,” Sci Data 3, 160035, 2016, https://doi.org/10.1038/sdata.2016.35\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}